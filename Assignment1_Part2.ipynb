{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment1 - Part2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/antonismaitis/textMining/blob/master/Assignment1_Part2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "e-gbxuE-OGHB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import re\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn import model_selection,metrics\n",
        "from sklearn.naive_bayes import MultinomialNB,BernoulliNB,GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
        "from sklearn.pipeline import make_pipeline \n",
        "from nltk.stem import PorterStemmer,WordNetLemmatizer\n",
        "\n",
        "\n",
        "\n",
        "#Create  Stemmer and Lemmatizer\n",
        "ps = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "\n",
        "#NLTK stopwords\n",
        "stopwords = set(stopwords.words(\"english\"))\n",
        "#after splitting every stopword in a new line manually,read as a txt and add to set stopwords1\n",
        "stopwordsstring = open('SentenceCorpus/word_lists/stopwords.txt',\"r\")\n",
        "\n",
        "stopwords1 = []\n",
        "for word in stopwordsstring:\n",
        "    if word.endswith('\\n'):\n",
        "        word = word[:-1]\n",
        "        stopwords1.append(word)\n",
        "#Stopwords1 list had 2 elements '', so i removed them    \n",
        "stopwords1.remove('')\n",
        "stopwords1.remove('')\n",
        "\n",
        "#Union of given stopwords set with nltk stopwords\n",
        "fstopwords = stopwords.union(stopwords1)\n",
        "\n",
        "#print(fstopwords)\n",
        "\n",
        "\n",
        "textfiles=[]\n",
        "\n",
        "x = os.listdir('SentenceCorpus/labeled_articles')\n",
        "for i in range(len(x)):\n",
        "    textfiles = []\n",
        "    for file in glob.iglob('SentenceCorpus/labeled_articles'+ '/*.txt'.format(i+1)):\n",
        "        textfiles.append(file)\n",
        "\n",
        "labels = []\n",
        "sentences = []\n",
        "filtered = []\n",
        "\n",
        "for file in textfiles:\n",
        "    g = open(file,\"r\")\n",
        "    lines = g.readlines()\n",
        "    for text in lines:\n",
        "        if  text.startswith('#'):\n",
        "            continue\n",
        "        elif \"\\t\" in text:  #Case where category and sentence are seperated with TAB\n",
        "            labels.append(text.split(\"\\t\")[0])\n",
        "            sentences.append(text.split(\"\\t\")[1].lower())\n",
        "        else: #Case where category and sentence are seperated with ' ' (space)\n",
        "            labels.append(text.split(\" \")[0])\n",
        "            sentences.append(text[len(text.split(\" \")[0])+1:].lower())\n",
        "\n",
        "#Stemming and Lemmatization decreased accuracy scores, while removing stopwords increased them about 10%\n",
        "            \n",
        "for i in range(len(sentences)):\n",
        "    if sentences[i].endswith(\"\\n\"): #One sentence per line \n",
        "        sentences[i] = sentences[i][:-1]\n",
        "    sentences[i] = \" \".join([word for word in word_tokenize(sentences[i]) if word not in fstopwords])\n",
        "    sentences[i] = re.sub(' +', ' ', sentences[i])\n",
        "    \n",
        "x_train,x_test,y_train,y_test = model_selection.train_test_split(sentences,labels,random_state=22,stratify = labels)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rhATKUWQPDnW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Big alpha - small accuracies / Small alpha - big accuracies\n",
        "\n",
        "#MultinomialNB - CountVectorizer\n",
        "\n",
        "alpha = 0.1 # This is the smoothing parameter for Laplace/Lidstone smoothing\n",
        "\n",
        "model = make_pipeline(CountVectorizer(ngram_range=(2,2), binary=True), MultinomialNB(alpha=alpha))\n",
        "\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "y_predicted = model.predict(x_test)\n",
        "\n",
        "\n",
        "recall = metrics.recall_score(y_test,y_predicted,average='macro')\n",
        "precision = metrics.precision_score(y_test,y_predicted,average='macro')\n",
        "f1 = metrics.f1_score(y_test,y_predicted,average='macro')\n",
        "\n",
        "print(\"Recall: %f\" % recall)\n",
        "print(\"Precision: %f\" % precision)\n",
        "print(\"F1: %f\" % f1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bn6Yo4SyPEK2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#MultinomialNB - TFIDFVectorizer\n",
        "\n",
        "alpha = 0.1\n",
        "\n",
        "model = make_pipeline(TfidfVectorizer(ngram_range = (3,3), binary = True), MultinomialNB(alpha = alpha))\n",
        "\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "y_predicted = model.predict(x_test)\n",
        "\n",
        "recall = metrics.recall_score(y_test,y_predicted,average='macro')\n",
        "precision = metrics.precision_score(y_test,y_predicted,average='macro')\n",
        "f1 = metrics.f1_score(y_test,y_predicted,average='macro')\n",
        "\n",
        "print(\"Recall: %f\" % recall)\n",
        "print(\"Precision: %f\" % precision)\n",
        "print(\"F1: %f\" % f1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DCcvA9gvPEgH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#BernoulliNB - TFIDFVectorizer\n",
        "\n",
        "alpha = 0.1\n",
        "\n",
        "model = make_pipeline(TfidfVectorizer(ngram_range = (1,1), binary = False), BernoulliNB(alpha = alpha))\n",
        "\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "y_predicted = model.predict(x_test)\n",
        "\n",
        "recall = metrics.recall_score(y_test,y_predicted,average='macro')\n",
        "precision = metrics.precision_score(y_test,y_predicted,average='macro')\n",
        "f1 = metrics.f1_score(y_test,y_predicted,average='macro')\n",
        "\n",
        "print(\"Recall: %f\" % recall)\n",
        "print(\"Precision: %f\" % precision)\n",
        "print(\"F1: %f\" % f1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bf7sRWJzPEzr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#BernoulliNB - CountVectorizer\n",
        "\n",
        "alpha = 0.1\n",
        "\n",
        "model = make_pipeline(CountVectorizer(ngram_range = (1,1), binary = False), BernoulliNB(alpha = alpha))\n",
        "\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "y_predicted = model.predict(x_test)\n",
        "\n",
        "recall = metrics.recall_score(y_test,y_predicted,average='macro')\n",
        "precision = metrics.precision_score(y_test,y_predicted,average='macro')\n",
        "f1 = metrics.f1_score(y_test,y_predicted,average='macro')\n",
        "\n",
        "print(\"Recall: %f\" % recall)\n",
        "print(\"Precision: %f\" % precision)\n",
        "print(\"F1: %f\" % f1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AeMZMvN3PFOi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}